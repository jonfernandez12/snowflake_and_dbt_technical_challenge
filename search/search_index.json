{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"faced_problems.html","title":"Faced problems","text":"<p>This section aims to gather the most relevant problems I faced during the  development of the technical challenge. However, I believe most of them were  already described in the Modus Operandi documentation.</p>"},{"location":"faced_problems.html#deploying-airflow","title":"Deploying Airflow","text":"<p>Deplying Airflow is a bit of a hustle having in mind that you need at least  three containers running as services (scheduler, webserver and executor) and  another two containers for databases (airflow and celery).</p>"},{"location":"faced_problems.html#raw-data-bulking","title":"Raw data bulking","text":"<p>When developing locally, it would be great to have access to some kind of  cloud storage to be able to replicate an end-to-end system behaviour without  the need of using big platforms that yes, have free tiers but also ask for you  card number.</p>"},{"location":"faced_problems.html#requirement-manager","title":"Requirement manager","text":"<p>I really like uv python manager is very fast indeed, however, I felt a bit lost  when trying to dump or compile the packages in uv to a requirements.txt file and making a 3rd party docker image not explode while trying to read this  requirements.txt. Maybe it gets solved by building myself the docker image but  for sure I was not able to fully fix this issue during the challenge.</p>"},{"location":"future_implementations.html","title":"Future implementation(s)","text":"<p>The following section aims to gather all those point I was not able to finish  and that I think it would be great to implement some day.</p> <ol> <li> <p>I would definitely not use <code>astro</code> and go for writing my own docker image for Airflow so as I can handle dependencies better and in order to know what is  going on behind the scenes.</p> </li> <li> <p>I would also like to have separate dependencies for developing the code and  code that is being pushed to production. There must be a way of defining those with uv as we can do with pipenv.</p> </li> <li> <p>I would be nice to have CI/CD pipelines that everytime we push code to the  repository, the run pytest and our linters. Also it would be great to have a job that creates a Github Page out of our documentation as in Github Pages. I tried but I think it does not work with private repositories</p> </li> <li> <p>I would like to add more tests as I already mentioned in the <code>Setting up tests</code> in the Modus Operandi documentation and maybe follow TDD somehow next time.</p> </li> </ol>"},{"location":"modus_operandi.html","title":"Modus Operandi","text":"<p>The following document aims to gather all the steps I thought about during  the development of the technical challenge.</p>"},{"location":"modus_operandi.html#landing-the-challenge","title":"Landing the challenge","text":"<p>First, I read the challenge several times to makes sure I understood the task.  At first glance, it does not seem a very complex task, also the datasets are  not very big so I suppose I will not have any issues processing the data.</p> <p>The final output is based on the join of patients with steps and exercises  accordingly and calculating the maximum sum of minutes (coming from steps and  exercises) grouped by patient.</p>"},{"location":"modus_operandi.html#infrastructure-discussion","title":"Infrastructure discussion","text":"<p>Since I am familiar with Airflow and dbt (I did no use it in 2 years so lets see what changed) and the challenge itself does not seem extremely  complex I would love to invest some time in creating a proper infrastructure:  - Dockerized Airflow and DBT with connection to Snowflake</p> <p>However, I have never deployed Airflow locally from scratch (everywhere I went, it was already there) so I will leave deploying dbt in Airflow for the end.</p> <p>I decided to use Snowflake as database based on: - I did not use it before and I want to learn a new DB engine. - It was listed in the nice-to-have list of the job offer.</p> <p>Also, I would like to implement data QA tests (table constraints, business criteria and schema checks), a CI/CD pipeline to automate python tests, add renovate bot to the repository and documentation deployment to the git project ,but we will see how it goes.</p> <p>Not sure if I will create a dashboard for the results, but it would be nice as  well. I have some ideas for other KPIs: - Apart from <code>total_minutes</code>, we can show as well the minutes coming from  steps and the ones coming from exercises, it could be a useful KPI to get more insights about the rehabilitation. - We can also split the maximum total minutes per country and, as in the  previous point, split it in exercises and steps. - Steps and exercises submission_time(s) graphs (respectively) , it would be great to know when (along the year) are usually the patients doing more steps and exercises, maybe it has some effect on their rehabilitation time.</p>"},{"location":"modus_operandi.html#setting-up-github","title":"Setting up Github","text":"<p>I am not sure if it is relevant but as I am using a laptop with an already paired Gitlab account, I had to create another SSH key and assign this new SSH  key to the Github account I am going to use for the challenge:</p> <pre><code>ssh-keygen -t ed25519 -C \"my@mail.com\"\nssh-add --apple-use-keychain ~/.ssh/my_id \nnano /Users/jonfernandez/.ssh/config\n</code></pre> <p>And here add:</p> <p><pre><code>  Host *\n    UseKeychain yes\n    AddKeysToAgent yes\n    IdentityFile ~/.ssh/my_id\n</code></pre> In Github SSH configuration section add the content of <code>cat ~/.ssh/my_id.pub</code> as always. Finally, and I think this changed recently, I had to log in with github-cli <code>gh auth login</code>.</p>"},{"location":"modus_operandi.html#setting-up-snowflake","title":"Setting up Snowflake","text":"<p>I am starting by setting up the Snowflake user for dbt as recommended in one of the Snowflake's quickstart guides.</p> <pre><code>USE ROLE SECURITYADMIN;\n\n-- We create the role that DBT user will use: dbt_developer_role \nCREATE OR REPLACE ROLE dbt_developer_role COMMENT='DBT developer role';\nGRANT ROLE dbt_developer_role TO ROLE SYSADMIN;\n\n-- Create the DBT user\nCREATE OR REPLACE USER dbt_user PASSWORD='dbt_password'\n    DEFAULT_ROLE=dbt_developer_role\n    DEFAULT_WAREHOUSE=dbt_warehouse\n    COMMENT='DBT User';\n\nGRANT ROLE dbt_developer_role TO USER dbt_user;\n\n-- To grant privileges to the role we need to use a role with higher permissions \nUSE ROLE ACCOUNTADMIN;\n\nGRANT CREATE DATABASE ON ACCOUNT TO ROLE dbt_developer_role;\n\nUSE ROLE SYSADMIN;\n\n-- Create Warehouse for DBT\nCREATE OR REPLACE WAREHOUSE dbt_developer_warehouse\n  WITH WAREHOUSE_SIZE = 'XSMALL'\n  AUTO_SUSPEND = 120\n  AUTO_RESUME = true\n  INITIALLY_SUSPENDED = TRUE;\n\nGRANT ALL ON WAREHOUSE dbt_developer_warehouse TO ROLE dbt_developer_role;\n</code></pre>"},{"location":"modus_operandi.html#early-data-model-approach","title":"Early data model approach","text":"<p>After that, we create the data model for our patients, exercises and steps. The assumptions and standards I chose to follow:</p> <ul> <li>We prefer plural from singular table namings so as SQL code is more intuitive: <code>SELECT * FROM patients;</code></li> <li>We prefer explicit over implicit type definition (i.e. we use<code>NUMBER(38,0)</code> instead of <code>INTEGER</code> so the amount of decimals is properly defined in the code even though <code>NUMBER(38,0)</code> is the standard in Snowflake for numeric data  types)</li> <li>Same goes for string data types, we will be using <code>VARCHAR(16777216)</code> which would be the same as using <code>VARCHAR</code> but we rather define the maximum length of the field in our code.</li> <li>It would be great to align with the team in charge of building the source of the data so as we can define properly the limits of the values in the columns  and use it as a second type validation.</li> <li>In exercises and steps tables, <code>external_id</code> column names will be modified to <code>patient_id</code>.</li> <li>For the timestamp columns, I used TIMESTAMP_TZ Snowflake type since, in the data, it looks like the UTC offset is defined after the timestamp  (i.e. <code>2024-04-11T14:25:23.708+0200</code>). However, we will need to take into  account that <code>Attention</code> section defined in the documentation when using this field for creating KPIs, since the offset of some countries change during the  year but not the value of the field. If possible I would ask the team in charge  of creating the source data to send us the values of the TIMEZONE together with the timestamp (without the UTC offset in this case) so as we can calculate  the UTC time in place when needed.</li> <li>In steps tables, <code>submission_time</code> column name will be modified  to <code>submitted_at</code>, like that all the columns with type <code>TIMESTAMP_TZ</code> will have the same suffix and we will be able to identify the type of the column by its  name.</li> <li>When trying to test how Snowflake is reading the timestamp values from the  spreadsheet <code>SELECT '2023-04-19T19:03:58.0520200'::TIMESTAMP_TZ</code> I got <code>Timestamp '2023-08-04T21:26:24.871+0200' is not recognized</code> so I tried:</li> </ul> <pre><code>ALTER SESSION SET TIMESTAMP_TZ_OUTPUT_FORMAT = 'YYYY-MM-DDTHH24:MI:SS.FF3TZHTZM';\n</code></pre> <p>It looks like it can read it know, we will take care of this when importing the data to Snowflake. </p> <p>The table creation script I used for the staging tables is the following: <pre><code>CREATE OR REPLACE DATABASE caspar_health;\nUSE ROLE dbt_developer_role;\n\nCREATE TABLE stg_patients (\n    row_id  NUMBER(38, 0),\n    patient_id NUMBER(38, 0),\n    first_name VARCHAR(16777216),\n    last_name VARCHAR(16777216),\n    country VARCHAR(16777216)\n);\nCREATE TABLE stg_exercises (\n    id NUMBER(38, 0),\n    external_id NUMBER(38, 0),\n    minutes NUMBER(38, 0),\n    completed_at VARCHAR(16777216),\n    updated_at VARCHAR(16777216)\n);\nCREATE TABLE stg_steps (\n    id NUMBER(38, 0),\n    external_id NUMBER(38, 0),\n    steps NUMBER(38, 0),\n    submission_time VARCHAR(16777216),\n    updated_at VARCHAR(16777216)\n);\n\nALTER SESSION SET TIMESTAMP_TZ_OUTPUT_FORMAT = 'YYYY-MM-DDTHH24:MI:SS.FF3TZHTZM';\n</code></pre></p>"},{"location":"modus_operandi.html#setting-up-dbt","title":"Setting up DBT","text":""},{"location":"modus_operandi.html#setting-up-the-python-environment","title":"Setting up the Python environment","text":"<p>I am going to be using uv as a python  package manager to start with the dbt dependencies. It is being a while since  I wanted to try uv out, is supposed to be very fast.</p> <pre><code>brew install uv\nuv init caspar_health_technical_challenge\nuv add dbt-core\nuv add dbt-snowflake\n</code></pre> <p>*Yes, it was fast indeed.</p> <p>I am not using <code>dbt-cloud</code> since looks expensive for what it offers and it is  not really complicated to set up and deploy <code>dbt-core</code> but maybe I regret it.  I run <code>dbt init</code> to create the dbt project. Then, <code>dbt deps</code> to install the  dependencies (<code>dbt-labs/dbt_utils</code>). The dbt profile would look as follows:</p> <pre><code>caspar_health_technical_challenge:\n  outputs:\n    dev:\n      account: RG94457.EU-CENTRAL-1\n      database: CASPAR_HEALTH\n      password: dbt_password\n      role: dbt_developer_role\n      schema: rehabilitation_data\n      threads: 1\n      type: snowflake\n      user: dbt_user\n      warehouse: dbt_developer_warehouse\n  target: dev\n</code></pre> <p>It took me some time to figure the Snowflake connection parameters out to  create the dbt profile, more specifically; - account: I had to run a query on Snowflake to get it. <pre><code>SELECT CONCAT_WS('.', CURRENT_ACCOUNT(), REPLACE(REGEXP_REPLACE(CURRENT_REGION(), '^[^_]+_',''), '_', '-')); -- e.g.: `YY00042.EU-CENTRAL-1`\n</code></pre> - password and user: I did not know if it was the Snowflake account or the  database specific account.</p> <p>I added a <code>generate_schema_name</code> and <code>set_query_tag</code> macros as recommended  in the Snowflake quickstart guide I am following.</p>"},{"location":"modus_operandi.html#loading-raw-data","title":"Loading raw data","text":"<p>After setting up dbt, I decided to try to import the raw data into Snowflake. I added the CSV files into the <code>seeds</code> folder and tried to run <code>dbt seed</code> but I got the following error:</p> <pre><code>Runtime Error\n  Database error while listing schemas in database \"CASPAR_HEALTH\"\n  Database Error\n    002043 (02000): SQL compilation error:\n    Object does not exist, or operation cannot be performed.\n</code></pre> <p>And since I could not see the query, I went to the logs, and apparently I was  trying to run <code>show objects in CASPAR_HEALTH.rehabilitation_data limit 10000</code> but, of course, <code>rehabilitation_data</code> schema does not exist, it should  be <code>public</code> instead. For some reason, I thought that DBT was creating a  new schema when adding data from seeds.</p> <p>I took the decision of adding the <code>stg</code> suffix to the tables names containing  raw data as specified in dbt documentation. I learned that, according to this  post, apparently <code>dbt seeds</code> is not the greatest option to bulk raw data into  Snowflake and, honestly, I would rather define a dbt model with COPY INTO table clause, but I do not have any personal cloud storage  account so I am going with dbt seeds for this very specific challenge.</p> <p>The data bulk worked with no issue for <code>steps</code> and <code>exercices</code>, however, for  <code>patients</code>, I had to use <code>--full-refresh</code> option (only for the very first time  we bulk the data) since the 1st column name is empty. Due to that, the 1st  column name for this table will have <code>A</code> as a column name instead of <code>row_id</code>  as planned. We could have avoided this by loading the data directly from a  cloud storage and just not selecting that column. I am not planning on using the column anyway in further tables, so it should not be an issue.</p>"},{"location":"modus_operandi.html#dimensions-and-facts","title":"Dimensions and facts","text":"<p>Once our raw data has been loaded, is time to discuss which  transformations we will be doing to our data.</p> <p>For this specific case and with no further knowledge about future  requirements, I would go with a simple dimensions and facts data model design where Patients will be the main dimension and Steps and Exercises the  facts that do not make sense without our dimension. The data model design  has not a big impact in this specific case apart from helping understand our  data and defining the primary keys (<code>id</code> in <code>patients</code>) and foreign keys  (<code>patient_id</code> in <code>steps</code> and <code>exercises</code>). </p> <p>This tables will be kept in <code>transform</code> schema (as defined in the Snowflake guide we are following) since they are part of the base layer of our model, and  they all should have a <code>seed</code> or raw data as source. In this state we will be taking care of the transformations mentioned in section Early data model approach and automatic dbt  data QA tests defined on the table constraints such as:     - primary keys (unique, not null)     - foreign keys (referenced to the origin)     - type validation</p> <p>I added <code>not null</code> constraint to almost every field in the data model because  the data shows that it is possible, however, it would be great to confirm  and truly redefine which columns expect <code>null</code> values and which do not.</p> <p>Since we do not have any constraint in one model that applies to multiple  columns, we will be only defining <code>column-level</code> constraints and not  <code>model-level</code> constraints. Also, we are going to use one file per model to  define the schema instead of defining all the schemas in the same file so  the code structure scales up in a clean and organized way.</p>"},{"location":"modus_operandi.html#analysis-tables","title":"Analysis tables","text":"<p>Finally, we are building two analysis tables and we will be using <code>analysis</code> schema for that.</p> <p>First, we will join patients data with steps and exercises data so as we can group it and sum it. We will also be creating the KPI <code>total_minutes</code>, a sum of the minutes coming from steps and exercises, so as we can get the maximum  value(s) in the last results table. And last but not list, we will create a  column with a <code>RANK()</code> window function ordered by <code>total_minutes</code> descendant,  which will return the same value in case the maximum value is repeated for  different patients.</p> <p>Second, we will obtain our aimed KPI filtering the rank field we created in the  previous table with <code>1</code>. </p> <pre><code>SELECT *\nFROM caspar_health.analysis.results_patients;\n</code></pre> patient_id:int first_name:str last_name:str country:str total_minutes:number(38,3) 356134 Austin Ellis Germany 54954235.000 <p>*_<code>total_minutes</code> column is decimal type (and it was proposed to be <code>int</code>   type) since one of the columns involved in the KPI had 3 decimals as well.</p> <p>I would love to create more KPIs as defined in Infrastructure discussion  section, but I am going to focus on deploying dbt in Airflow, adding  python pytest tests, sqlfluff, ruff and proper documentation first.</p>"},{"location":"modus_operandi.html#setting-up-airflow","title":"Setting up Airflow","text":"<p>Following the Snowflake guide we might need to get rid of uv for setting up python requirements and go with plain pip requirement management. We will now prepare the repository for  encapsulating our model creation with dbt inside an airflow repository  ready to be deployed.</p> <p>We can create a dockerized airflow repository by creating a new folder and  running <code>astro dev init</code> (from astro), after that I will just add all my dbt packages inside <code>dags/dbt/caspar_health_dbt_cosmos</code> folder.</p>"},{"location":"modus_operandi.html#setting-up-the-dag","title":"Setting up the DAG","text":"<p>The <code>patient_data_elt</code> DAG is only composed by: 1. Dbt profile definition: Which basically, thanks to a very cool cosmos library, reads all the credentials from the predefined Airflow connections in  <code>airflow_settings.yaml</code>. Given the use case, I am not going to spend more time  trying to hide the secrets since in a real-life scenario I would just use  Airflow AWS Secrets Manager Backend for getting the credentials of the connections. 2. Dbt DAG: We create tasks that run every existing model in DBT, even the seeds creating a <code>DbtDag</code> module.</p> <p></p>"},{"location":"modus_operandi.html#astro-and-cosmos","title":"Astro and cosmos","text":"<p>I did not use <code>astro</code> and <code>cosmos</code> until today and is crazy how easy they make: - Astro: It deploys the webserver, the scheduler and the trigger  (I guess this is the Celery queue). I just did not need to think about it,  which is nice for this use case. However, I can imagine that those layers of  abstraction that adds on top of Airflow might be a bit cumbersome when we want to adjust Airflow to some specific needs. Also, it took me quite some time to discover how to deploy the connections defined on <code>airflow_settings.yaml</code>:  <pre><code>astro dev object import\nastro dev restart\n</code></pre></p> <ul> <li>Cosmos: I really do not have any complains about it for now but again, it  could be not the best option if we want to really get the most out of dbt.</li> </ul>"},{"location":"modus_operandi.html#setting-up-tests-tndd","title":"Setting up tests (TNDD)","text":"<p>Well, I would have loved to apply TDD during the development of these tasks,  however, I got excited by the use of uv at the beginning and I forgot about it. That is why we will can call it Test Non Driven Development in this case.</p> <p>I did not add any python tests myself since I did not see the case for it,  maybe it would have been great to: - Test somehow the connection with Snowflake (with a given dbt profile) - Test the output of the dbt models somehow.</p> <p>What I did add is sqlfluff and ruff, which are very nice SQL and python linters that help so much following predefined code standards.</p>"},{"location":"statement.html","title":"Goal of the project","text":"<p>The goal of this project is to showcase an end to end ELT pipeline from a data source to any data warehouse/data source using Python, SQL and Git to answer the following question:</p> <ul> <li>Find the patient(s) with the most generated minutes.</li> </ul> <p>In order to answer this question, you\u2019ve been provided with the following three datasets: - steps.csv   - ID: table ID   - External_id: patient_id   - Steps: number of generated steps per submission time   - Submission_time: the time when the steps are submitted   - Updated_at: last time when the row was updated - exercises.csv   - ID: table ID   - External_id: patient_id   - minutes: total duration of an exercise in minutes   - completed_at: the time when the exercise was completed   - Updated_at: last time when the row was updated - patients.csv   - Patient_id   - First_name   - Last_name   -  Country</p> <p>To solve the problem above, these are the business rules, assumptions and some handy tips: - As you have probably already noticed, in the steps.csv dataset, there is no column with the completed minutes, but rather a column with the submitted steps. That\u2019s fine, as there is a business requirement saying how to convert steps into minutes using the following formula -&gt;</p> <p><code>minutes = steps * 0.002</code></p> <ul> <li>A single patient can submit steps multiple times, and can complete multiple exercises.</li> <li>Please use Python only for data extraction and injection, and use SQL for data manipulation.</li> <li>Have in mind that multiple patients can generate the same amount of minutes, meaning that the output table can in theory have more than 1 row</li> <li>The output should have the following columns</li> </ul> patient_id:int first_name:str last_name:str country:str total_minutes:int <p>Feel free to be creative and if you have knowledge of any of the following technologies (Docker, cloud services, AirFlow, DBT), feel free to use them as well in your solution. If not, no worries, you will learn them at Caspar :)</p>"},{"location":"statement.html#deploy-airflow-environemnt","title":"Deploy Airflow environemnt","text":"<pre><code>make start\n</code></pre>"}]}